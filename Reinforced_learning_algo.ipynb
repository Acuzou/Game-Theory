{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "529657da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym\n",
    "#pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3058e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a15f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d451a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, {'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[110, 109, 106],\n",
       "        [110, 109, 106],\n",
       "        [124, 122, 122],\n",
       "        ...,\n",
       "        [108, 111, 109],\n",
       "        [108, 111, 109],\n",
       "        [118, 119, 119]],\n",
       "\n",
       "       [[110, 109, 106],\n",
       "        [110, 109, 106],\n",
       "        [124, 122, 122],\n",
       "        ...,\n",
       "        [108, 111, 109],\n",
       "        [108, 111, 109],\n",
       "        [118, 119, 119]],\n",
       "\n",
       "       [[114, 116, 115],\n",
       "        [114, 116, 115],\n",
       "        [126, 127, 126],\n",
       "        ...,\n",
       "        [112, 113, 111],\n",
       "        [112, 113, 111],\n",
       "        [118, 117, 115]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[116, 115, 116],\n",
       "        [116, 115, 116],\n",
       "        [106, 107, 108],\n",
       "        ...,\n",
       "        [113, 115, 114],\n",
       "        [113, 115, 114],\n",
       "        [117, 114, 117]],\n",
       "\n",
       "       [[116, 115, 116],\n",
       "        [116, 115, 116],\n",
       "        [106, 107, 108],\n",
       "        ...,\n",
       "        [113, 115, 114],\n",
       "        [113, 115, 114],\n",
       "        [117, 114, 117]],\n",
       "\n",
       "       [[115, 112, 112],\n",
       "        [115, 112, 112],\n",
       "        [119, 119, 117],\n",
       "        ...,\n",
       "        [123, 119, 118],\n",
       "        [123, 119, 118],\n",
       "        [114, 114, 117]]], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(state)\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f473e27",
   "metadata": {},
   "source": [
    "# Possible Actions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c7e9456",
   "metadata": {},
   "source": [
    "down (0), up (1), right (2), left (3), pick-up (4), and drop-off (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f90473a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93a3abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227, -1, False, False, {'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)})\n"
     ]
    }
   ],
   "source": [
    "print(env.step(env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa736f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(341, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98a5f22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4463012",
   "metadata": {},
   "source": [
    "# Random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecf7f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 1240 Steps with a total reward of -4738\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "counter = 0\n",
    "g = 0\n",
    "reward = None\n",
    "while reward != 20:\n",
    "    state, reward, done, _bool, info = env.step(env.action_space.sample())\n",
    "    counter += 1\n",
    "    g += reward\n",
    "print(\"Solved in {} Steps with a total reward of {}\".format(counter, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd83e56",
   "metadata": {},
   "source": [
    "# Reinforced Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d14da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48b42d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros([n_states, n_actions])\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5639d068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To keep history of Q values\n",
    "Q_hist = np.zeros([n_states, n_actions])\n",
    "Q_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5b3c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.618\n",
    "G = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8383de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "print(env.reset()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "349cd2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State = (41, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Initial State = (13, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n",
      "Initial State = (33, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})\n",
      "Initial State = (327, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Initial State = (103, {'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)})\n",
      "Final State = (475, {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 1, 0], dtype=int8)})\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1, episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0, 0\n",
    "    state_info = env.reset()\n",
    "    firstState = state_info\n",
    "    print(\"Initial State = {}\".format(firstState))\n",
    "    \n",
    "    while reward != 20:\n",
    "        action = np.argmax(Q[state_info[0]])\n",
    "        next_state, reward, done, _bool, info = env.step(action)\n",
    "        Q[state_info[0], action] += alpha * (reward + np.max(Q[next_state]) - Q[state_info[0], action])\n",
    "        G += reward\n",
    "        state_info = (next_state, info)\n",
    "        \n",
    "        #To keep a history a Q values in a multi dimensional array\n",
    "        Q_hist = np.dstack(Q)\n",
    "finalState = state_info\n",
    "print(\"Final State = {}\".format(finalState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5253e815",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-5c0d333a1bd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "#To render agent choices for optimal Q\n",
    "state = env.reset()\n",
    "done = None\n",
    "while done != True:\n",
    "    action = np.argmax(Q[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0de12",
   "metadata": {},
   "source": [
    "## Let's add reward discouting and a decaying epsilon for randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b18e8f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-43ee0c3fc41c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m     \"\"\"\n\u001b[1;32m-> 1193\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilon = 1\n",
    "gamma = 0.95\n",
    "\n",
    "rewardTracker = []\n",
    "Q = np.zeros([n_states, n_states])\n",
    "episodes = 5000\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0, 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while done != True:\n",
    "        \n",
    "        if np.random.rand() > epsilon:\n",
    "            action = np.argmax(Q[state[0]])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            epsilon -= 10**3\n",
    "            \n",
    "        if action == 6:\n",
    "            action = 0\n",
    "            \n",
    "        state2, reward, done, _bool, info = env.step(action)\n",
    "        Q[state[0], action] += alpha * (reward + gamma * np.max(Q[state2]) - Q[state[0], action])\n",
    "        G += reward\n",
    "        state = (state2, info)\n",
    "    \n",
    "    rewardTracker.append(G)\n",
    "        \n",
    "average_end = sum(rewardTracker[episodes-100:episodes]) / 100.0\n",
    "if average_end > 70:\n",
    "    print(\"----------------------------\")\n",
    "    print(\"Solved after {} episodes with average return of {}\".format(episodes-100, average_end))\n",
    "\n",
    "print(\"Average return of {}\".format(sum(rewardTracker)/len(rewardTracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(eps, Q, state, episode):\n",
    "    if np.random.rand() > eps:\n",
    "        action = np.argmax(Q[state, :] + np.random.randn(1, n_actions)/(episode/4))\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        eps -= 10**-5\n",
    "        \n",
    "    return action, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not yet adapted to formal case for this python version\n",
    "#To define optimal hyper parameters\n",
    "def learn_Q(alpha, gamma, eps, numTrainingEpisodes, numTrainingSteps):\n",
    "    global Q_star\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    rewardTracker = []\n",
    "    \n",
    "    for episode in range(1, numTrainingEpisodes+1):\n",
    "        G = 0\n",
    "        state = env.reset()\n",
    "        \n",
    "        for step in range(1, numTrainingSteps):\n",
    "            \n",
    "            action, eps = e_greedy(eps, Q, state, episode)\n",
    "            state2, reward, done, info = env.step(action)\n",
    "            Q[state, action] += alpha*( reward + gamme * np.max(Q[state2]) - Q[state, action])\n",
    "            state = state2\n",
    "            G += reward\n",
    "            \n",
    "        rewardTracker.append(Q)\n",
    "    \n",
    "        if (episode % (numTrainingEpisodes*.10) == 0 and episode != 0):\n",
    "            print(\"Alpha {} Gamma {} Epsilon {:04.3f} Episode {} of {}\".format(alpha, gamma, eps, episode, numTrainingEpisode))\n",
    "            print(\"Average Total Return: {}\".format(sum(rewardTracker)/episode))\n",
    "            \n",
    "        average_end = sum(rewardTracker[episode-100:episode]) / 100.0\n",
    "        if average_end > 70:\n",
    "            print(\"----------------------------\")\n",
    "            print(\"Solved after {} episodes with average return of {}\".format(episode-100, average_end))\n",
    "            Q_star = Q\n",
    "            break\n",
    "    Q_start = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328099b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
